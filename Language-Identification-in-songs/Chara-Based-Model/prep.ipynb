{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics Text Normalization\n",
    "\n",
    "This script reads an Excel file containing lyrics data, normalizes the text in the lyrics, and saves the modified data back to an Excel file.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Read the Excel file**: The pandas library is used to read the Excel file containing the lyrics data. The file path is specified in the code.\n",
    "\n",
    "2. **Normalize text**: A function `normalize_text` is defined to normalize the text in the lyrics. This function converts the text to lowercase, normalizes whitespace, and keeps only certain punctuation marks. It uses the `unicodedata` library to categorize characters.\n",
    "\n",
    "3. **Apply the normalization function**: The `normalize_text` function is applied to the 'Lyrics' column of the DataFrame. The normalized texts are stored in a new column 'NormalizedText'.\n",
    "\n",
    "4. **Save the modified DataFrame**: The modified DataFrame, which now includes the normalized text, is saved to an Excel file. The output file path is specified in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel('../Chara-Based-Model/DataSets/Lyrics Training Data GPT Generated.xlsx', engine='openpyxl')\n",
    "\n",
    "# Normalize text\n",
    "def normalize_text(texts):\n",
    "    normalized_texts = []\n",
    "    keep_punctuation = {\"'\", \"-\", \"’\"}  # Add or remove characters as needed\n",
    "    for text in texts:\n",
    "        try:\n",
    "            text = str(text).lower()  # Convert to lowercase and ensure text is string\n",
    "            text = ' '.join(text.split())  # Normalize whitespace\n",
    "            # Iterate over each character in the text\n",
    "            # and include it in the output if it meets certain conditions\n",
    "            text = ''.join(\n",
    "                char for char in text \n",
    "                if unicodedata.category(char)[0] in ('L', 'N', 'Z')  # Check if the character is a letter, number, or space\n",
    "                or char in keep_punctuation  # Or if the character is in the custom set of punctuation marks to keep\n",
    "            )\n",
    "\n",
    "            normalized_texts.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {text} with error {e}\")\n",
    "            normalized_texts.append(text)  # Append original text or handle as needed\n",
    "    return normalized_texts\n",
    "\n",
    "\n",
    "# Normalize text\n",
    "normalized_texts = normalize_text(df['Lyrics'].astype(str))\n",
    "\n",
    "# Add the normalized text to the dataframe\n",
    "df['NormalizedText'] = normalized_texts\n",
    "\n",
    "# Save the modified DataFrame\n",
    "output_path = 'normalized_text.xlsx'  # Replace with a path that is not synced\n",
    "df.to_excel(output_path, engine='openpyxl', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing and Label Binarization in Python\n",
    "\n",
    "This script reads an Excel file containing normalized text data, processes the text, converts language labels into a binary matrix, and writes the processed data back to a new Excel file.\n",
    "\n",
    "## Libraries Used\n",
    "- pandas: For data manipulation and analysis.\n",
    "- sklearn.preprocessing: Provides a utility class MultiLabelBinarizer for transforming multiclass labels to binary labels.\n",
    "\n",
    "## Steps\n",
    "1. The required libraries are imported.\n",
    "2. The normalized Excel file is read into a pandas DataFrame.\n",
    "3. The 'Languages' column of the DataFrame is processed to convert the comma-separated string of languages into a list of languages.\n",
    "4. The MultiLabelBinarizer is initialized.\n",
    "5. The language labels are converted into a binary matrix.\n",
    "6. A new DataFrame is created for the binary matrix with appropriate column names.\n",
    "7. The original DataFrame is concatenated with the new binary matrix DataFrame.\n",
    "8. The resulting DataFrame is written to a new Excel file.\n",
    "\n",
    "Please ensure that the required libraries are installed in your Python environment before running this script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame saved to combined_normalized_text.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_excel(\"./normalized_text.xlsx\")  # Update the path to where you've stored the file\n",
    "\n",
    "# Step 2: Process the 'Languages' column to handle multiple labels\n",
    "# Split the string on commas to get a list of languages for each entry\n",
    "# Correctly process the 'Langauges' column to ensure it contains lists of languages without extra characters\n",
    "df['Langauges'] = df['Langauges'].apply(lambda x: [lang.strip() for lang in x.split(',') if lang.strip()])\n",
    "\n",
    "# Reinitialize MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Reconvert language labels into a binary matrix format\n",
    "binary_matrix = mlb.fit_transform(df['Langauges'])\n",
    "\n",
    "# Reconvert the binary matrix back into a DataFrame for easy viewing/manipulation\n",
    "binary_matrix_df_corrected = pd.DataFrame(binary_matrix, columns=mlb.classes_)\n",
    "\n",
    "# Inspect the corrected DataFrame\n",
    "binary_matrix_df_corrected.head()\n",
    "\n",
    "\n",
    "# Merge the binary matrix with the original DataFrame\n",
    "df_combined = pd.concat([df, binary_matrix_df_corrected], axis=1)\n",
    "\n",
    "# Save the combined DataFrame to a new Excel file\n",
    "output_file_path = \"combined_normalized_text.xlsx\"  # Specify your desired output file path\n",
    "df_combined.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Combined DataFrame saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AR,EN,FR' 'DE,ES,KO' 'EN,HI,RU' 'EN,ES,PT' 'EN,FR,IT' 'EN,FR' 'AR,FR'\n",
      " 'EN,KO' 'ES,IT' 'EN,SW' 'DE,EN' 'AR,EN' 'EN,ES' 'EN,IS' 'ES,PT']\n"
     ]
    }
   ],
   "source": [
    "print(df['Langauges'].apply(lambda x: ','.join(sorted(x))).unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters (including 'UNK'): 660\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel('combined_normalized_text.xlsx', engine='openpyxl')\n",
    "normalized_texts = df['NormalizedText'].tolist()\n",
    "\n",
    "# Concatenate all normalized texts into one large string to find unique characters\n",
    "all_text = ''.join(normalized_texts)\n",
    "\n",
    "# Identify and sort unique characters\n",
    "unique_chars = sorted(set(all_text))\n",
    "\n",
    "# Create a mapping from unique characters to indices, adding an 'UNK' (unknown) token for unseen characters\n",
    "char_to_index = {'UNK': 0}  # Start with 'UNK' token mapped to 0\n",
    "char_to_index.update({char: index + 1 for index, char in enumerate(unique_chars)})  # Shift indices by 1\n",
    "\n",
    "# Create a reverse mapping from indices to characters\n",
    "index_to_char = {index: char for char, index in char_to_index.items()}\n",
    "\n",
    "# Display the number of unique characters, including the 'UNK' token\n",
    "print(f\"Total unique characters (including 'UNK'): {len(char_to_index)}\")\n",
    "\n",
    "# Optional: Save the mappings to JSON files for future use\n",
    "with open('char_to_index.json', 'w') as f:\n",
    "    json.dump(char_to_index, f)\n",
    "\n",
    "with open('index_to_char.json', 'w') as f:\n",
    "    json.dump(index_to_char, f)\n",
    "\n",
    "# The character vocabulary is now created and saved. This includes handling of unseen characters via 'UNK' token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to Index Mapping:\n",
      "{' ': 1, \"'\": 2, '-': 3, '0': 4, '1': 5, '2': 6, '3': 7, '6': 8, '9': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'q': 26, 'r': 27, 's': 28, 't': 29, 'u': 30, 'v': 31, 'w': 32, 'x': 33, 'y': 34, 'z': 35, 'ß': 36, 'à': 37, 'á': 38, 'â': 39, 'ã': 40, 'ä': 41, 'æ': 42, 'ç': 43, 'è': 44, 'é': 45, 'ê': 46, 'í': 47, 'î': 48, 'ï': 49, 'ñ': 50, 'ó': 51, 'ô': 52, 'ö': 53, 'ù': 54, 'ú': 55, 'û': 56, 'ü': 57, 'œ': 58, 'а': 59, 'б': 60, 'в': 61, 'г': 62, 'д': 63, 'е': 64, 'ж': 65, 'з': 66, 'и': 67, 'й': 68, 'к': 69, 'л': 70, 'м': 71, 'н': 72, 'о': 73, 'п': 74, 'р': 75, 'с': 76, 'т': 77, 'у': 78, 'х': 79, 'ц': 80, 'ч': 81, 'ш': 82, 'щ': 83, 'ъ': 84, 'ы': 85, 'ь': 86, 'ю': 87, 'я': 88, 'ё': 89, 'ء': 90, 'آ': 91, 'أ': 92, 'إ': 93, 'ئ': 94, 'ا': 95, 'ب': 96, 'ة': 97, 'ت': 98, 'ث': 99, 'ج': 100, 'ح': 101, 'خ': 102, 'د': 103, 'ذ': 104, 'ر': 105, 'ز': 106, 'س': 107, 'ش': 108, 'ص': 109, 'ض': 110, 'ط': 111, 'ظ': 112, 'ع': 113, 'غ': 114, 'ف': 115, 'ق': 116, 'ك': 117, 'ل': 118, 'م': 119, 'ن': 120, 'ه': 121, 'و': 122, 'ى': 123, 'ي': 124, 'ڨ': 125, 'अ': 126, 'आ': 127, 'इ': 128, 'ई': 129, 'उ': 130, 'ए': 131, 'ओ': 132, 'औ': 133, 'क': 134, 'ख': 135, 'ग': 136, 'घ': 137, 'च': 138, 'छ': 139, 'ज': 140, 'ट': 141, 'ड': 142, 'ण': 143, 'त': 144, 'थ': 145, 'द': 146, 'ध': 147, 'न': 148, 'प': 149, 'फ': 150, 'ब': 151, 'भ': 152, 'म': 153, 'य': 154, 'र': 155, 'ल': 156, 'व': 157, 'श': 158, 'ष': 159, 'स': 160, 'ह': 161, '’': 162, '가': 163, '각': 164, '간': 165, '갇': 166, '갈': 167, '감': 168, '강': 169, '같': 170, '개': 171, '거': 172, '걱': 173, '건': 174, '걷': 175, '걸': 176, '검': 177, '겁': 178, '것': 179, '게': 180, '겠': 181, '겨': 182, '격': 183, '견': 184, '겸': 185, '경': 186, '계': 187, '고': 188, '곤': 189, '곧': 190, '골': 191, '곳': 192, '공': 193, '과': 194, '관': 195, '광': 196, '괜': 197, '괴': 198, '교': 199, '구': 200, '굳': 201, '굴': 202, '그': 203, '근': 204, '금': 205, '기': 206, '긴': 207, '길': 208, '김': 209, '까': 210, '깔': 211, '깜': 212, '깨': 213, '깰': 214, '꺼': 215, '꺾': 216, '께': 217, '껴': 218, '꼈': 219, '꼬': 220, '꼭': 221, '꼴': 222, '꽃': 223, '꾸': 224, '꿈': 225, '꿔': 226, '끄': 227, '끌': 228, '끔': 229, '끝': 230, '끼': 231, '끽': 232, '나': 233, '낙': 234, '난': 235, '날': 236, '남': 237, '낯': 238, '내': 239, '낼': 240, '냐': 241, '냥': 242, '너': 243, '넌': 244, '널': 245, '넘': 246, '넣': 247, '네': 248, '노': 249, '녹': 250, '놀': 251, '높': 252, '놓': 253, '놔': 254, '누': 255, '눈': 256, '눴': 257, '느': 258, '는': 259, '늘': 260, '능': 261, '니': 262, '닌': 263, '님': 264, '다': 265, '닥': 266, '달': 267, '담': 268, '답': 269, '당': 270, '닿': 271, '대': 272, '댈': 273, '더': 274, '던': 275, '데': 276, '도': 277, '독': 278, '돌': 279, '돼': 280, '됐': 281, '되': 282, '된': 283, '두': 284, '둘': 285, '둠': 286, '둥': 287, '둬': 288, '뒤': 289, '뒷': 290, '드': 291, '득': 292, '든': 293, '들': 294, '듯': 295, '디': 296, '따': 297, '딱': 298, '땅': 299, '때': 300, '땐': 301, '떠': 302, '떤': 303, '떨': 304, '또': 305, '똑': 306, '뚫': 307, '뛰': 308, '뜨': 309, '뜻': 310, '띠': 311, '라': 312, '락': 313, '란': 314, '랄': 315, '람': 316, '랐': 317, '랑': 318, '래': 319, '러': 320, '런': 321, '럴': 322, '럼': 323, '럽': 324, '렇': 325, '레': 326, '렘': 327, '려': 328, '력': 329, '렵': 330, '렸': 331, '로': 332, '록': 333, '롭': 334, '루': 335, '뤄': 336, '르': 337, '른': 338, '를': 339, '름': 340, '리': 341, '린': 342, '릴': 343, '림': 344, '릿': 345, '링': 346, '마': 347, '막': 348, '만': 349, '말': 350, '맘': 351, '맛': 352, '맞': 353, '매': 354, '맹': 355, '머': 356, '먹': 357, '먼': 358, '멀': 359, '멋': 360, '메': 361, '멘': 362, '며': 363, '면': 364, '모': 365, '목': 366, '몫': 367, '몰': 368, '몸': 369, '못': 370, '묘': 371, '무': 372, '묵': 373, '문': 374, '물': 375, '뭐': 376, '미': 377, '믿': 378, '밀': 379, '바': 380, '반': 381, '발': 382, '밝': 383, '밟': 384, '밤': 385, '방': 386, '백': 387, '버': 388, '번': 389, '법': 390, '베': 391, '벽': 392, '변': 393, '별': 394, '볕': 395, '보': 396, '복': 397, '본': 398, '볼': 399, '봄': 400, '봐': 401, '봤': 402, '부': 403, '분': 404, '불': 405, '비': 406, '빗': 407, '빛': 408, '빠': 409, '빨': 410, '빴': 411, '빼': 412, '뻐': 413, '뻗': 414, '쁨': 415, '사': 416, '삭': 417, '살': 418, '삶': 419, '상': 420, '새': 421, '색': 422, '생': 423, '서': 424, '선': 425, '설': 426, '성': 427, '세': 428, '센': 429, '셋': 430, '셔': 431, '소': 432, '속': 433, '손': 434, '쇠': 435, '쇼': 436, '수': 437, '숙': 438, '순': 439, '술': 440, '숨': 441, '숲': 442, '쉬': 443, '쉽': 444, '슈': 445, '스': 446, '슴': 447, '습': 448, '시': 449, '신': 450, '실': 451, '심': 452, '싶': 453, '싸': 454, '싼': 455, '써': 456, '썩': 457, '쓰': 458, '쓸': 459, '아': 460, '악': 461, '안': 462, '앉': 463, '않': 464, '알': 465, '앗': 466, '았': 467, '앞': 468, '애': 469, '야': 470, '약': 471, '양': 472, '얘': 473, '어': 474, '억': 475, '언': 476, '얼': 477, '없': 478, '엊': 479, '에': 480, '엔': 481, '엤': 482, '여': 483, '연': 484, '열': 485, '엽': 486, '였': 487, '영': 488, '옆': 489, '예': 490, '옛': 491, '오': 492, '온': 493, '올': 494, '옷': 495, '와': 496, '왔': 497, '외': 498, '요': 499, '용': 500, '우': 501, '운': 502, '울': 503, '움': 504, '웃': 505, '워': 506, '원': 507, '위': 508, '유': 509, '으': 510, '은': 511, '을': 512, '음': 513, '의': 514, '이': 515, '익': 516, '인': 517, '일': 518, '잃': 519, '임': 520, '입': 521, '있': 522, '잊': 523, '자': 524, '작': 525, '잔': 526, '잖': 527, '잘': 528, '잠': 529, '잡': 530, '장': 531, '재': 532, '저': 533, '적': 534, '전': 535, '점': 536, '정': 537, '제': 538, '젠': 539, '져': 540, '졌': 541, '조': 542, '족': 543, '좀': 544, '좋': 545, '주': 546, '죽': 547, '준': 548, '줄': 549, '중': 550, '쥔': 551, '즐': 552, '즘': 553, '지': 554, '직': 555, '진': 556, '질': 557, '집': 558, '짓': 559, '짖': 560, '짜': 561, '짝': 562, '쩌': 563, '쩔': 564, '쯤': 565, '찍': 566, '차': 567, '찬': 568, '찮': 569, '참': 570, '찾': 571, '채': 572, '챙': 573, '처': 574, '척': 575, '천': 576, '첫': 577, '쳐': 578, '초': 579, '추': 580, '축': 581, '춤': 582, '춰': 583, '취': 584, '치': 585, '친': 586, '침': 587, '카': 588, '캄': 589, '컴': 590, '컷': 591, '켜': 592, '콘': 593, '콜': 594, '콤': 595, '쿵': 596, '크': 597, '큰': 598, '큼': 599, '키': 600, '타': 601, '탈': 602, '탐': 603, '태': 604, '터': 605, '털': 606, '테': 607, '토': 608, '툭': 609, '트': 610, '특': 611, '틀': 612, '틈': 613, '파': 614, '판': 615, '팔': 616, '팝': 617, '팰': 618, '퍼': 619, '펀': 620, '펼': 621, '포': 622, '표': 623, '푸': 624, '풀': 625, '품': 626, '픈': 627, '필': 628, '핑': 629, '하': 630, '한': 631, '할': 632, '함': 633, '항': 634, '해': 635, '햇': 636, '했': 637, '행': 638, '향': 639, '험': 640, '헤': 641, '혀': 642, '혔': 643, '혼': 644, '홀': 645, '홍': 646, '화': 647, '환': 648, '활': 649, '황': 650, '회': 651, '후': 652, '훅': 653, '흐': 654, '흔': 655, '흥': 656, '희': 657, '히': 658, '힘': 659}\n",
      "\n",
      "Index to Character Mapping:\n",
      "{1: ' ', 2: \"'\", 3: '-', 4: '0', 5: '1', 6: '2', 7: '3', 8: '6', 9: '9', 10: 'a', 11: 'b', 12: 'c', 13: 'd', 14: 'e', 15: 'f', 16: 'g', 17: 'h', 18: 'i', 19: 'j', 20: 'k', 21: 'l', 22: 'm', 23: 'n', 24: 'o', 25: 'p', 26: 'q', 27: 'r', 28: 's', 29: 't', 30: 'u', 31: 'v', 32: 'w', 33: 'x', 34: 'y', 35: 'z', 36: 'ß', 37: 'à', 38: 'á', 39: 'â', 40: 'ã', 41: 'ä', 42: 'æ', 43: 'ç', 44: 'è', 45: 'é', 46: 'ê', 47: 'í', 48: 'î', 49: 'ï', 50: 'ñ', 51: 'ó', 52: 'ô', 53: 'ö', 54: 'ù', 55: 'ú', 56: 'û', 57: 'ü', 58: 'œ', 59: 'а', 60: 'б', 61: 'в', 62: 'г', 63: 'д', 64: 'е', 65: 'ж', 66: 'з', 67: 'и', 68: 'й', 69: 'к', 70: 'л', 71: 'м', 72: 'н', 73: 'о', 74: 'п', 75: 'р', 76: 'с', 77: 'т', 78: 'у', 79: 'х', 80: 'ц', 81: 'ч', 82: 'ш', 83: 'щ', 84: 'ъ', 85: 'ы', 86: 'ь', 87: 'ю', 88: 'я', 89: 'ё', 90: 'ء', 91: 'آ', 92: 'أ', 93: 'إ', 94: 'ئ', 95: 'ا', 96: 'ب', 97: 'ة', 98: 'ت', 99: 'ث', 100: 'ج', 101: 'ح', 102: 'خ', 103: 'د', 104: 'ذ', 105: 'ر', 106: 'ز', 107: 'س', 108: 'ش', 109: 'ص', 110: 'ض', 111: 'ط', 112: 'ظ', 113: 'ع', 114: 'غ', 115: 'ف', 116: 'ق', 117: 'ك', 118: 'ل', 119: 'م', 120: 'ن', 121: 'ه', 122: 'و', 123: 'ى', 124: 'ي', 125: 'ڨ', 126: 'अ', 127: 'आ', 128: 'इ', 129: 'ई', 130: 'उ', 131: 'ए', 132: 'ओ', 133: 'औ', 134: 'क', 135: 'ख', 136: 'ग', 137: 'घ', 138: 'च', 139: 'छ', 140: 'ज', 141: 'ट', 142: 'ड', 143: 'ण', 144: 'त', 145: 'थ', 146: 'द', 147: 'ध', 148: 'न', 149: 'प', 150: 'फ', 151: 'ब', 152: 'भ', 153: 'म', 154: 'य', 155: 'र', 156: 'ल', 157: 'व', 158: 'श', 159: 'ष', 160: 'स', 161: 'ह', 162: '’', 163: '가', 164: '각', 165: '간', 166: '갇', 167: '갈', 168: '감', 169: '강', 170: '같', 171: '개', 172: '거', 173: '걱', 174: '건', 175: '걷', 176: '걸', 177: '검', 178: '겁', 179: '것', 180: '게', 181: '겠', 182: '겨', 183: '격', 184: '견', 185: '겸', 186: '경', 187: '계', 188: '고', 189: '곤', 190: '곧', 191: '골', 192: '곳', 193: '공', 194: '과', 195: '관', 196: '광', 197: '괜', 198: '괴', 199: '교', 200: '구', 201: '굳', 202: '굴', 203: '그', 204: '근', 205: '금', 206: '기', 207: '긴', 208: '길', 209: '김', 210: '까', 211: '깔', 212: '깜', 213: '깨', 214: '깰', 215: '꺼', 216: '꺾', 217: '께', 218: '껴', 219: '꼈', 220: '꼬', 221: '꼭', 222: '꼴', 223: '꽃', 224: '꾸', 225: '꿈', 226: '꿔', 227: '끄', 228: '끌', 229: '끔', 230: '끝', 231: '끼', 232: '끽', 233: '나', 234: '낙', 235: '난', 236: '날', 237: '남', 238: '낯', 239: '내', 240: '낼', 241: '냐', 242: '냥', 243: '너', 244: '넌', 245: '널', 246: '넘', 247: '넣', 248: '네', 249: '노', 250: '녹', 251: '놀', 252: '높', 253: '놓', 254: '놔', 255: '누', 256: '눈', 257: '눴', 258: '느', 259: '는', 260: '늘', 261: '능', 262: '니', 263: '닌', 264: '님', 265: '다', 266: '닥', 267: '달', 268: '담', 269: '답', 270: '당', 271: '닿', 272: '대', 273: '댈', 274: '더', 275: '던', 276: '데', 277: '도', 278: '독', 279: '돌', 280: '돼', 281: '됐', 282: '되', 283: '된', 284: '두', 285: '둘', 286: '둠', 287: '둥', 288: '둬', 289: '뒤', 290: '뒷', 291: '드', 292: '득', 293: '든', 294: '들', 295: '듯', 296: '디', 297: '따', 298: '딱', 299: '땅', 300: '때', 301: '땐', 302: '떠', 303: '떤', 304: '떨', 305: '또', 306: '똑', 307: '뚫', 308: '뛰', 309: '뜨', 310: '뜻', 311: '띠', 312: '라', 313: '락', 314: '란', 315: '랄', 316: '람', 317: '랐', 318: '랑', 319: '래', 320: '러', 321: '런', 322: '럴', 323: '럼', 324: '럽', 325: '렇', 326: '레', 327: '렘', 328: '려', 329: '력', 330: '렵', 331: '렸', 332: '로', 333: '록', 334: '롭', 335: '루', 336: '뤄', 337: '르', 338: '른', 339: '를', 340: '름', 341: '리', 342: '린', 343: '릴', 344: '림', 345: '릿', 346: '링', 347: '마', 348: '막', 349: '만', 350: '말', 351: '맘', 352: '맛', 353: '맞', 354: '매', 355: '맹', 356: '머', 357: '먹', 358: '먼', 359: '멀', 360: '멋', 361: '메', 362: '멘', 363: '며', 364: '면', 365: '모', 366: '목', 367: '몫', 368: '몰', 369: '몸', 370: '못', 371: '묘', 372: '무', 373: '묵', 374: '문', 375: '물', 376: '뭐', 377: '미', 378: '믿', 379: '밀', 380: '바', 381: '반', 382: '발', 383: '밝', 384: '밟', 385: '밤', 386: '방', 387: '백', 388: '버', 389: '번', 390: '법', 391: '베', 392: '벽', 393: '변', 394: '별', 395: '볕', 396: '보', 397: '복', 398: '본', 399: '볼', 400: '봄', 401: '봐', 402: '봤', 403: '부', 404: '분', 405: '불', 406: '비', 407: '빗', 408: '빛', 409: '빠', 410: '빨', 411: '빴', 412: '빼', 413: '뻐', 414: '뻗', 415: '쁨', 416: '사', 417: '삭', 418: '살', 419: '삶', 420: '상', 421: '새', 422: '색', 423: '생', 424: '서', 425: '선', 426: '설', 427: '성', 428: '세', 429: '센', 430: '셋', 431: '셔', 432: '소', 433: '속', 434: '손', 435: '쇠', 436: '쇼', 437: '수', 438: '숙', 439: '순', 440: '술', 441: '숨', 442: '숲', 443: '쉬', 444: '쉽', 445: '슈', 446: '스', 447: '슴', 448: '습', 449: '시', 450: '신', 451: '실', 452: '심', 453: '싶', 454: '싸', 455: '싼', 456: '써', 457: '썩', 458: '쓰', 459: '쓸', 460: '아', 461: '악', 462: '안', 463: '앉', 464: '않', 465: '알', 466: '앗', 467: '았', 468: '앞', 469: '애', 470: '야', 471: '약', 472: '양', 473: '얘', 474: '어', 475: '억', 476: '언', 477: '얼', 478: '없', 479: '엊', 480: '에', 481: '엔', 482: '엤', 483: '여', 484: '연', 485: '열', 486: '엽', 487: '였', 488: '영', 489: '옆', 490: '예', 491: '옛', 492: '오', 493: '온', 494: '올', 495: '옷', 496: '와', 497: '왔', 498: '외', 499: '요', 500: '용', 501: '우', 502: '운', 503: '울', 504: '움', 505: '웃', 506: '워', 507: '원', 508: '위', 509: '유', 510: '으', 511: '은', 512: '을', 513: '음', 514: '의', 515: '이', 516: '익', 517: '인', 518: '일', 519: '잃', 520: '임', 521: '입', 522: '있', 523: '잊', 524: '자', 525: '작', 526: '잔', 527: '잖', 528: '잘', 529: '잠', 530: '잡', 531: '장', 532: '재', 533: '저', 534: '적', 535: '전', 536: '점', 537: '정', 538: '제', 539: '젠', 540: '져', 541: '졌', 542: '조', 543: '족', 544: '좀', 545: '좋', 546: '주', 547: '죽', 548: '준', 549: '줄', 550: '중', 551: '쥔', 552: '즐', 553: '즘', 554: '지', 555: '직', 556: '진', 557: '질', 558: '집', 559: '짓', 560: '짖', 561: '짜', 562: '짝', 563: '쩌', 564: '쩔', 565: '쯤', 566: '찍', 567: '차', 568: '찬', 569: '찮', 570: '참', 571: '찾', 572: '채', 573: '챙', 574: '처', 575: '척', 576: '천', 577: '첫', 578: '쳐', 579: '초', 580: '추', 581: '축', 582: '춤', 583: '춰', 584: '취', 585: '치', 586: '친', 587: '침', 588: '카', 589: '캄', 590: '컴', 591: '컷', 592: '켜', 593: '콘', 594: '콜', 595: '콤', 596: '쿵', 597: '크', 598: '큰', 599: '큼', 600: '키', 601: '타', 602: '탈', 603: '탐', 604: '태', 605: '터', 606: '털', 607: '테', 608: '토', 609: '툭', 610: '트', 611: '특', 612: '틀', 613: '틈', 614: '파', 615: '판', 616: '팔', 617: '팝', 618: '팰', 619: '퍼', 620: '펀', 621: '펼', 622: '포', 623: '표', 624: '푸', 625: '풀', 626: '품', 627: '픈', 628: '필', 629: '핑', 630: '하', 631: '한', 632: '할', 633: '함', 634: '항', 635: '해', 636: '햇', 637: '했', 638: '행', 639: '향', 640: '험', 641: '헤', 642: '혀', 643: '혔', 644: '혼', 645: '홀', 646: '홍', 647: '화', 648: '환', 649: '활', 650: '황', 651: '회', 652: '후', 653: '훅', 654: '흐', 655: '흔', 656: '흥', 657: '희', 658: '히', 659: '힘'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Load the normalized text data\n",
    "    df = pd.read_excel('normalized_text.xlsx', engine='openpyxl')\n",
    "except FileNotFoundError:\n",
    "    print(\"The specified file does not exist.\")\n",
    "    \n",
    "\n",
    "normalized_texts = df['NormalizedText'].tolist()  # Assuming the column name is 'NormalizedText'\n",
    "\n",
    "# Concatenate all normalized texts into one large string\n",
    "all_text = ''.join(normalized_texts)\n",
    "\n",
    "# Identify and sort unique characters\n",
    "unique_chars = sorted(set(all_text))\n",
    "\n",
    "# Create a mapping from unique characters to indices\n",
    "char_to_index = {char: index for index, char in enumerate(unique_chars, start=1)}  # Start indexing from 1\n",
    "\n",
    "# Optional: Create a reverse mapping from indices to characters\n",
    "index_to_char = {index: char for char, index in char_to_index.items()}\n",
    "\n",
    "# Display the mappings\n",
    "print(\"Character to Index Mapping:\")\n",
    "print(char_to_index)\n",
    "print(\"\\nIndex to Character Mapping:\")\n",
    "print(index_to_char)\n",
    "\n",
    "# Save the mappings for future use\n",
    "with open('char_to_index.json', 'w') as f:\n",
    "    json.dump(char_to_index, f)\n",
    "with open('index_to_char.json', 'w') as f:\n",
    "    json.dump(index_to_char, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (826934811.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb()\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the normalized text data (make sure to update the path to where your normalized texts are stored)\n",
    "# df = pd.read_excel('normalized_text.xlsx', engine='openpyxl')\n",
    "# normalized_texts = df['NormalizedText'].tolist()  # Assuming the column name is 'NormalizedText'\n",
    "\n",
    "# # Concatenate all normalized texts into one large string\n",
    "# all_text = ''.join(normalized_texts)\n",
    "\n",
    "# # Identify and sort unique characters\n",
    "# unique_chars = sorted(set(all_text))\n",
    "\n",
    "# # Create a mapping from unique characters to indices\n",
    "# char_to_index = {char: index for index, char in enumerate(unique_chars, start=1)}  # Start indexing from 1\n",
    "\n",
    "# # Optional: Create a reverse mapping from indices to characters\n",
    "# index_to_char = {index: char for char, index in char_to_index.items()}\n",
    "\n",
    "# # Display the mappings\n",
    "# print(\"Character to Index Mapping:\")\n",
    "# print(char_to_index)\n",
    "# print(\"\\nIndex to Character Mapping:\")\n",
    "# print(index_to_char)\n",
    "# # Save the mappings for future use\n",
    "# # You might want to save these mappings to a file or a database, depending on your project needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ramzidaher/OneDrive/Desktop/[02] University/Third Year/Individual Project/DataSets/Lyrics Training Data GPT Generated.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39municodedata\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Install the required libraries by running this in your Python environment:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# pip install pandas openpyxl\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Read the Excel file\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39m'\u001b[39;49m\u001b[39m/home/ramzidaher/OneDrive/Desktop/[02] University/Third Year/Individual Project/DataSets/Lyrics Training Data GPT Generated.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m, engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mopenpyxl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Assume the text is in a column named 'Text'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ramzidaher/Desktop/Language-Identification-in-songs/Chara-Based-Model/prep.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m texts \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mLyrics\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    481\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[1;32m    483\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[1;32m    484\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_base.py:1695\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m=\u001b[39m engine\n\u001b[1;32m   1693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorage_options \u001b[39m=\u001b[39m storage_options\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engines[engine](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_io, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_openpyxl.py:557\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[39mReader using openpyxl engine.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39m{storage_options}\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mopenpyxl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 557\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(filepath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_base.py:535\u001b[0m, in \u001b[0;36mBaseExcelReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m IOHandles(\n\u001b[1;32m    532\u001b[0m     handle\u001b[39m=\u001b[39mfilepath_or_buffer, compression\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m}\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_or_buffer, (ExcelFile, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workbook_class)):\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m    536\u001b[0m         filepath_or_buffer, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    537\u001b[0m     )\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workbook_class):\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbook \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ramzidaher/OneDrive/Desktop/[02] University/Third Year/Individual Project/DataSets/Lyrics Training Data GPT Generated.xlsx'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/python3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/ramzidaher/.local/lib/python3.10/site-packages (2.15.0.post1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: packaging in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: setuptools in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (69.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (1.60.1)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ramzidaher/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ramzidaher/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ramzidaher/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ramzidaher/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ramzidaher/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ramzidaher/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ramzidaher/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ramzidaher/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ramzidaher/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ramzidaher/.local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "# Use this executable path to install TensorFlow\n",
    "!{sys.executable} -m pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Using cached wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-69.1.0-py3-none-any.whl (819 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.60.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Collecting packaging\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.5\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.28.0-py2.py3-none-any.whl (186 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, opt-einsum, ml-dtypes, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: libclang\n",
      "    Found existing installation: libclang 16.0.6\n",
      "    Uninstalling libclang-16.0.6:\n",
      "      Successfully uninstalled libclang-16.0.6\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.5.26\n",
      "    Uninstalling flatbuffers-23.5.26:\n",
      "      Successfully uninstalled flatbuffers-23.5.26\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.42.0\n",
      "    Uninstalling wheel-0.42.0:\n",
      "      Successfully uninstalled wheel-0.42.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.4.0\n",
      "    Uninstalling termcolor-2.4.0:\n",
      "      Successfully uninstalled termcolor-2.4.0\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.36.0\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.36.0:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.36.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.15.0\n",
      "    Uninstalling tensorflow-estimator-2.15.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.1.0\n",
      "    Uninstalling setuptools-69.1.0:\n",
      "      Successfully uninstalled setuptools-69.1.0\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.1\n",
      "    Uninstalling pyasn1-0.5.1:\n",
      "      Successfully uninstalled pyasn1-0.5.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.2.2\n",
      "    Uninstalling oauthlib-3.2.2:\n",
      "      Successfully uninstalled oauthlib-3.2.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 3.5.2\n",
      "    Uninstalling Markdown-3.5.2:\n",
      "      Successfully uninstalled Markdown-3.5.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.60.1\n",
      "    Uninstalling grpcio-1.60.1:\n",
      "      Successfully uninstalled grpcio-1.60.1\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.4\n",
      "    Uninstalling gast-0.5.4:\n",
      "      Successfully uninstalled gast-0.5.4\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.2\n",
      "    Uninstalling cachetools-5.3.2:\n",
      "      Successfully uninstalled cachetools-5.3.2\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.1.0\n",
      "    Uninstalling absl-py-2.1.0:\n",
      "      Successfully uninstalled absl-py-2.1.0\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 3.0.1\n",
      "    Uninstalling Werkzeug-3.0.1:\n",
      "      Successfully uninstalled Werkzeug-3.0.1\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.9\n",
      "    Uninstalling rsa-4.9:\n",
      "      Successfully uninstalled rsa-4.9\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyasn1-modules\n",
      "    Found existing installation: pyasn1-modules 0.3.0\n",
      "    Uninstalling pyasn1-modules-0.3.0:\n",
      "      Successfully uninstalled pyasn1-modules-0.3.0\n",
      "  Attempting uninstall: opt-einsum\n",
      "    Found existing installation: opt-einsum 3.3.0\n",
      "    Uninstalling opt-einsum-3.3.0:\n",
      "      Successfully uninstalled opt-einsum-3.3.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.10.0\n",
      "    Uninstalling h5py-3.10.0:\n",
      "      Successfully uninstalled h5py-3.10.0\n",
      "  Attempting uninstall: google-pasta\n",
      "    Found existing installation: google-pasta 0.2.0\n",
      "    Uninstalling google-pasta-0.2.0:\n",
      "      Successfully uninstalled google-pasta-0.2.0\n",
      "  Attempting uninstall: astunparse\n",
      "    Found existing installation: astunparse 1.6.3\n",
      "    Uninstalling astunparse-1.6.3:\n",
      "      Successfully uninstalled astunparse-1.6.3\n",
      "  Attempting uninstall: requests-oauthlib\n",
      "    Found existing installation: requests-oauthlib 1.3.1\n",
      "    Uninstalling requests-oauthlib-1.3.1:\n",
      "      Successfully uninstalled requests-oauthlib-1.3.1\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.28.0\n",
      "    Uninstalling google-auth-2.28.0:\n",
      "      Successfully uninstalled google-auth-2.28.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.2.0\n",
      "    Uninstalling google-auth-oauthlib-1.2.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.2\n",
      "    Uninstalling tensorboard-2.15.2:\n",
      "      Successfully uninstalled tensorboard-2.15.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0.post1\n",
      "    Uninstalling tensorflow-2.15.0.post1:\n",
      "      Successfully uninstalled tensorflow-2.15.0.post1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flask-sqlalchemy 3.1.1 requires sqlalchemy>=2.0.16, but you have sqlalchemy 1.4.50 which is incompatible.\n",
      "botocore 1.34.5 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.2 certifi-2024.2.2 charset-normalizer-3.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.28.0 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.60.1 h5py-3.10.0 idna-3.6 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 numpy-1.26.4 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.2 protobuf-4.25.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 setuptools-69.1.0 six-1.16.0 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 typing-extensions-4.9.0 urllib3-2.2.1 werkzeug-3.0.1 wheel-0.42.0 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade --force-reinstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
